
https://github.com/tauri-apps/tauri/issues/10422

ollama serve
ollama run llama3.2:1b

curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt":"Why is the sky blue?"
}'


import ollama
response = ollama.chat(model='llama2-uncensored', messages=[
  {
    'role': 'system',
    'content': 'You are an AI assistant.',
  },
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])


stablelm2

